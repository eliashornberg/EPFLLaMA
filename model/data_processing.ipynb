{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix format for M1 preference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preference data\n",
    "path = 'datasets/M1_preference_data_15052024.json'\n",
    "data = utils.read_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data on the corerct format for DPO training\n",
    "def process_data(data):\n",
    "    processed_data = []  # This will hold the processed data\n",
    "\n",
    "    for item in data:\n",
    "        question = item[\"question_complete\"]\n",
    "        for pref in item[\"preference\"]:\n",
    "            chosen = pref['A'] if pref[\"overall\"] == 'A' else pref['B']\n",
    "            rejected = pref['B'] if pref[\"overall\"] == 'A' else pref['A']\n",
    "            processed_data.append({\n",
    "                \"prompt\": question,\n",
    "                \"chosen\": chosen,\n",
    "                \"rejected\": rejected\n",
    "            })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "data = process_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to jsonl file with correct data format\n",
    "utils.write_jsonl(data, 'datasets/M1.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix format for SFT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/intents.json'\n",
    "data = utils.read_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_intents(data):\n",
    "    transformed_data = []\n",
    "    for intent in data[\"intents\"]:\n",
    "        prompt = intent[\"patterns\"][0]\n",
    "        gold_output = intent[\"responses\"][0]\n",
    "        transformed_data.append({\"Prompt\": prompt, \"gold_output\": gold_output})\n",
    "    return transformed_data\n",
    "\n",
    "# Transform the original JSON data\n",
    "intents_data = process_intents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to jsonl file with correct data format\n",
    "utils.write_jsonl(intents_data, 'datasets/cs_sft.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets/math_stack_exchange.json'\n",
    "data = utils.read_json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_math(data):\n",
    "    transformed_data = []\n",
    "    for item in data:\n",
    "        prompt = item[\"question\"]\n",
    "        gold_output = item[\"chosen\"]\n",
    "        transformed_data.append({\"Prompt\": prompt, \"gold_output\": gold_output})\n",
    "    return transformed_data\n",
    "\n",
    "math_data = process_math(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all sft data\n",
    "sft_data = intents_data + math_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to jsonl file with correct data format\n",
    "utils.write_jsonl(sft_data, 'datasets/sft.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack exchange script to get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully processed and saved to datasets/processed_DS_SE_cleaned.json\n",
      "Data successfully processed and saved to datasets/processed_MATH_SE_cleaned.json\n",
      "Data successfully processed and saved to datasets/processed_CS_SE_cleaned.json\n",
      "Data successfully processed and saved to datasets/processed_PHYSICS_SE_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_parquet_to_json(parquet_file_path, json_file_path):\n",
    "    # Read the Parquet file\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    \n",
    "    # Convert the DataFrame to a JSON string\n",
    "    json_str = df.to_json(orient='records')\n",
    "    \n",
    "    # Write the JSON string to a file\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    cleaned_text = ''\n",
    "    for elem in soup.recursiveChildGenerator():\n",
    "        if elem.name == 'code':  # Preserve LaTeX equations\n",
    "            cleaned_text += f\"${elem.get_text()}$\"\n",
    "        elif isinstance(elem, str):\n",
    "            cleaned_text += elem\n",
    "    return cleaned_text\n",
    "\n",
    "def process_file(parquet_file, json_output_file, cleaned_output_file):\n",
    "    # Convert Parquet to JSON\n",
    "    convert_parquet_to_json(parquet_file, json_output_file)\n",
    "\n",
    "    # Load the JSON data\n",
    "    with open(json_output_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Prepare the new JSON structure\n",
    "    new_data = []\n",
    "\n",
    "    for item in data:\n",
    "        question = item['question']\n",
    "        answers = item['answers']\n",
    "\n",
    "        # Find the answer with the highest pm_score\n",
    "        best_answer = max(answers, key=lambda x: x['pm_score'])\n",
    "\n",
    "        # Only keep the answer if pm_score is >= 4\n",
    "        if best_answer['pm_score'] >= 4:\n",
    "            # Clean the HTML content\n",
    "            cleaned_question = clean_html(question)\n",
    "            cleaned_answer = clean_html(best_answer['text'])\n",
    "\n",
    "            # Create the new JSON structure\n",
    "            new_item = {\n",
    "                \"prompt\": cleaned_question,\n",
    "                \"gold_output\": cleaned_answer\n",
    "            }\n",
    "            new_data.append(new_item)\n",
    "\n",
    "    # Save the new JSON data to a file\n",
    "    with open(cleaned_output_file, 'w') as f:\n",
    "        json.dump(new_data, f, indent=4)\n",
    "\n",
    "    print(f\"Data successfully processed and saved to {cleaned_output_file}\")\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [\n",
    "    (\"datasets/DS_SE.parquet\", \"datasets/DS_SE.json\", \"datasets/processed_DS_SE_cleaned.json\"),\n",
    "    (\"datasets/MATH_SE.parquet\", \"datasets/MATH_SE.json\", \"datasets/processed_MATH_SE_cleaned.json\"),\n",
    "    (\"datasets/CS_SE.parquet\", \"datasets/CS_SE.json\", \"datasets/processed_CS_SE_cleaned.json\"),\n",
    "    (\"datasets/PHYSICS_SE.parquet\", \"datasets/PHYSICS_SE.json\", \"datasets/processed_PHYSICS_SE_cleaned.json\")\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for parquet_file, json_output_file, cleaned_output_file in files_to_process:\n",
    "    process_file(parquet_file, json_output_file, cleaned_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "data = []\n",
    "for path in [\"datasets/processed_DS_SE_cleaned.json\", \"datasets/processed_MATH_SE_cleaned.json\",\n",
    "             \"datasets/processed_CS_SE_cleaned.json\", \"datasets/processed_PHYSICS_SE_cleaned.json\"] :\n",
    "\n",
    "    data = data + utils.read_json(path)\n",
    "\n",
    "utils.write_jsonl(data, 'datasets/sft.jsonl')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKING LENGTHS AND CLEANING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "import utils\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "_, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name_or_path,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = False, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "def calculate_dpo_lengths(data, tokenizer):\n",
    "    lengths = []\n",
    "    for item in data:\n",
    "        prompt_tokens = len(tokenizer(item[\"prompt\"], truncation=False)['input_ids'])\n",
    "        chosen_tokens = len(tokenizer(item[\"chosen\"], truncation=False)['input_ids'])\n",
    "        rejected_tokens = len(tokenizer(item[\"rejected\"], truncation=False)['input_ids'])\n",
    "        max_length = max(prompt_tokens + chosen_tokens, prompt_tokens + rejected_tokens)\n",
    "        lengths.append((max_length, item))\n",
    "    return lengths\n",
    "\n",
    "# Tokenize and calculate lengths for SFT data\n",
    "def calculate_sft_lengths(data, tokenizer):\n",
    "    lengths = []\n",
    "    for item in data:\n",
    "        prompt_tokens = len(tokenizer(item[\"prompt\"], truncation=False)['input_ids'])\n",
    "        chosen_tokens = len(tokenizer(item[\"gold_output\"], truncation=False)['input_ids'])\n",
    "        length = prompt_tokens + chosen_tokens\n",
    "        lengths.append((length, item))\n",
    "    return lengths\n",
    "\n",
    "# Read the DPO and SFT data using utils\n",
    "dpo_data = utils.read_jsonl(\"datasets/M1.jsonl\")\n",
    "sft_data = utils.read_jsonl(\"datasets/sft.jsonl\")\n",
    "\n",
    "# Process DPO data and calculate lengths\n",
    "dpo_lengths = calculate_dpo_lengths(dpo_data, tokenizer)\n",
    "\n",
    "# Calculate lengths for SFT data\n",
    "sft_lengths = calculate_sft_lengths(sft_data, tokenizer)\n",
    "\n",
    "# Filter data points not exceeding 2048 tokens\n",
    "filtered_dpo_data = [item for length, item in dpo_lengths if length <= 2048]\n",
    "filtered_sft_data = [item for length, item in sft_lengths if length <= 2048]\n",
    "\n",
    "# Write filtered data to new JSONL files using utils\n",
    "utils.write_jsonl(filtered_dpo_data, \"datasets/M1_2048.jsonl\")\n",
    "utils.write_jsonl(filtered_sft_data, \"datasets/sft_2048.jsonl\")\n",
    "\n",
    "# Generate histograms\n",
    "dpo_lengths_values = [length for length, _ in dpo_lengths]\n",
    "sft_lengths_values = [length for length, _ in sft_lengths]\n",
    "\n",
    "plt.hist(dpo_lengths_values, bins=100, alpha=0.5, label='DPO lengths', range=(-1000, 5000))\n",
    "plt.hist(sft_lengths_values, bins=100, alpha=0.5, label='SFT lengths', range=(-1000, 5000))\n",
    "plt.xlabel('Length in tokens')\n",
    "plt.ylabel('Number of data points')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Token Length Distribution')\n",
    "plt.xlim(-1000, 5000)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage of data points exceeding 2048 tokens\n",
    "dpo_exceeding_2048 = sum(1 for length in dpo_lengths_values if length > 2048) / len(dpo_lengths_values) * 100\n",
    "sft_exceeding_2048 = sum(1 for length in sft_lengths_values if length > 2048) / len(sft_lengths_values) * 100\n",
    "\n",
    "print(f\"Percentage of DPO data points exceeding 2048 tokens: {dpo_exceeding_2048:.2f}%\")\n",
    "print(f\"Percentage of SFT data points exceeding 2048 tokens: {sft_exceeding_2048:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Skipping entry with special characters: ...\n",
      "Processed JSONL file saved as datasets/M1_2000_no_mcqa_train_correct.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = 'datasets/M1_2000_no_mcqa_train.jsonl'\n",
    "output_file = 'datasets/M1_2000_no_mcqa_train_correct.jsonl'\n",
    "\n",
    "# The strings to be removed\n",
    "start_string = \"<|system|>\\nYou are an experienced teacher who answers the STEM-related question asked by a student below.</s>\\n<|user|>\\n\"\n",
    "end_string = \"</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "special_chars_pattern = re.compile(r'^[\\W_]+$')\n",
    "\n",
    "# Function to process each prompt\n",
    "def process_prompt(prompt):\n",
    "    if prompt.startswith(start_string):\n",
    "        prompt = prompt[len(start_string):]\n",
    "\n",
    "    if prompt.endswith(end_string):\n",
    "        prompt = prompt[:-len(end_string)]\n",
    "    return prompt\n",
    "\n",
    "# Read the input file and process each line\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        # Skip entries where \"chosen\" field contains only special characters\n",
    "        if special_chars_pattern.match(data['chosen'].strip()):\n",
    "            print(f\"Skipping entry with special characters: {data['chosen']}\")\n",
    "            continue\n",
    "        \n",
    "        # Process the \"prompt\" field\n",
    "        data['prompt'] = process_prompt(data['prompt'])\n",
    "        \n",
    "        # Write the modified data to the output file\n",
    "        json.dump(data, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "print(f\"Processed JSONL file saved as {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SE_DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_parquet_to_json(parquet_file_path, json_file_path):\n",
    "    # Read the Parquet file\n",
    "    df = pd.read_parquet(parquet_file_path)\n",
    "    \n",
    "    # Convert the DataFrame to a JSON string\n",
    "    json_str = df.to_json(orient='records')\n",
    "    \n",
    "    # Write the JSON string to a file\n",
    "    with open(json_file_path, 'w') as f:\n",
    "        f.write(json_str)\n",
    "\n",
    "def clean_html(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    cleaned_text = ''\n",
    "    for elem in soup.recursiveChildGenerator():\n",
    "        if elem.name == 'code':\n",
    "            # Preserve LaTeX equations\n",
    "            cleaned_text += f\"${elem.get_text()}$\"\n",
    "        elif isinstance(elem, str):\n",
    "            cleaned_text += elem\n",
    "    return cleaned_text\n",
    "\n",
    "def process_file(parquet_file, json_output_file, cleaned_output_file):\n",
    "    # Convert Parquet to JSON\n",
    "    convert_parquet_to_json(parquet_file, json_output_file)\n",
    "    \n",
    "    # Load the JSON data\n",
    "    with open(json_output_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Prepare the new JSONL data\n",
    "    new_data = []\n",
    "    for item in data:\n",
    "        question = item['question']\n",
    "        answers = item['answers']\n",
    "        \n",
    "        # Skip if there is only one answer\n",
    "        if len(answers) <= 1:\n",
    "            continue\n",
    "        \n",
    "        # Find the answer with the highest pm_score\n",
    "        best_answer = max(answers, key=lambda x: x['pm_score'])\n",
    "        \n",
    "        # Find the answer with the lowest pm_score\n",
    "        worst_answer = min(answers, key=lambda x: x['pm_score'])\n",
    "        \n",
    "        # Only keep the data if best_answer pm_score is > 2\n",
    "        if best_answer['pm_score'] > 2:\n",
    "            # Clean the HTML content\n",
    "            cleaned_question = clean_html(question)\n",
    "            cleaned_best_answer = clean_html(best_answer['text'])\n",
    "            cleaned_worst_answer = clean_html(worst_answer['text'])\n",
    "            \n",
    "            # Create the new JSONL structure\n",
    "            new_item = {\n",
    "                \"prompt\": cleaned_question,\n",
    "                \"chosen\": cleaned_best_answer,\n",
    "                \"rejected\": cleaned_worst_answer\n",
    "            }\n",
    "            new_data.append(json.dumps(new_item))\n",
    "    \n",
    "    # Save the new JSONL data to a file\n",
    "    with open(cleaned_output_file, 'w') as f:\n",
    "        f.write('\\n'.join(new_data))\n",
    "    \n",
    "    print(f\"Data successfully processed and saved to {cleaned_output_file}\")\n",
    "\n",
    "# List of files to process\n",
    "files_to_process = [\n",
    "    (\"datasets/DS_SE.parquet\", \"datasets/DS_SE_DPO.json\", \"datasets/processed_DS_SE_cleaned_DPO.jsonl\"),\n",
    "    (\"datasets/MATH_SE.parquet\", \"datasets/MATH_SE_DPO.json\", \"datasets/processed_MATH_SE_cleaned_DPO.jsonl\"),\n",
    "    (\"datasets/CS_SE.parquet\", \"datasets/CS_SE_DPO.json\", \"datasets/processed_CS_SE_cleaned_DPO.jsonl\"),\n",
    "    (\"datasets/PHYSICS_SE.parquet\", \"datasets/PHYSICS_SE_DPO.json\", \"datasets/processed_PHYSICS_SE_cleaned_DPO.jsonl\")\n",
    "]\n",
    "\n",
    "# Process each file\n",
    "for parquet_file, json_output_file, cleaned_output_file in files_to_process:\n",
    "    process_file(parquet_file, json_output_file, cleaned_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Set the input and output file paths\n",
    "input_file = \"datasets/SE_DPO_.jsonl\"\n",
    "train_file = \"datasets/SE_DPO_train.jsonl\"\n",
    "test_file = \"datasets/SE_DPO_test.jsonl\"\n",
    "\n",
    "# Set the split ratio (95% for training, 5% for testing)\n",
    "train_ratio = 0.95\n",
    "\n",
    "# Read the data from the input JSONL file\n",
    "with open(input_file, \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Shuffle the data randomly\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(len(data) * train_ratio)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "# Write the training data to the train JSONL file\n",
    "with open(train_file, \"w\") as file:\n",
    "    for item in train_data:\n",
    "        file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Write the testing data to the test JSONL file\n",
    "with open(test_file, \"w\") as file:\n",
    "    for item in test_data:\n",
    "        file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Split complete. Training data: {len(train_data)} items, Testing data: {len(test_data)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "data = []\n",
    "for path in [\"datasets/processed_DS_SE_cleaned_DPO.jsonl\", \"datasets/processed_MATH_SE_cleaned_DPO.jsonl\",\n",
    "             \"datasets/processed_CS_SE_cleaned_DPO.jsonl\", \"datasets/processed_PHYSICS_SE_cleaned_DPO.jsonl\"] :\n",
    "\n",
    "    data = data + utils.read_jsonl(path)\n",
    "\n",
    "utils.write_jsonl(data, 'datasets/SE_DPO_.jsonl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "import utils\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "_, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name_or_path,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = False, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "def calculate_dpo_lengths(data, tokenizer):\n",
    "    lengths = []\n",
    "    for item in data:\n",
    "        prompt_tokens = len(tokenizer(item[\"prompt\"], truncation=False)['input_ids'])\n",
    "        chosen_tokens = len(tokenizer(item[\"chosen\"], truncation=False)['input_ids'])\n",
    "        rejected_tokens = len(tokenizer(item[\"rejected\"], truncation=False)['input_ids'])\n",
    "        max_length = max(prompt_tokens + chosen_tokens, prompt_tokens + rejected_tokens)\n",
    "        lengths.append((max_length, item))\n",
    "    return lengths\n",
    "\n",
    "# Tokenize and calculate lengths for SFT data\n",
    "def calculate_sft_lengths(data, tokenizer):\n",
    "    lengths = []\n",
    "    for item in data:\n",
    "        prompt_tokens = len(tokenizer(item[\"prompt\"], truncation=False)['input_ids'])\n",
    "        chosen_tokens = len(tokenizer(item[\"gold_output\"], truncation=False)['input_ids'])\n",
    "        length = prompt_tokens + chosen_tokens\n",
    "        lengths.append((length, item))\n",
    "    return lengths\n",
    "\n",
    "# Read the DPO and SFT data using utils\n",
    "dpo_data = utils.read_jsonl(\"datasets/M1.jsonl\")\n",
    "sft_data = utils.read_jsonl(\"datasets/sft.jsonl\")\n",
    "\n",
    "# Process DPO data and calculate lengths\n",
    "dpo_lengths = calculate_dpo_lengths(dpo_data, tokenizer)\n",
    "\n",
    "# Calculate lengths for SFT data\n",
    "sft_lengths = calculate_sft_lengths(sft_data, tokenizer)\n",
    "\n",
    "# Filter data points not exceeding 2048 tokens\n",
    "filtered_dpo_data = [item for length, item in dpo_lengths if length <= 2048]\n",
    "filtered_sft_data = [item for length, item in sft_lengths if length <= 2048]\n",
    "\n",
    "# Write filtered data to new JSONL files using utils\n",
    "utils.write_jsonl(filtered_dpo_data, \"datasets/M1_2048.jsonl\")\n",
    "utils.write_jsonl(filtered_sft_data, \"datasets/sft_2048.jsonl\")\n",
    "\n",
    "# Generate histograms\n",
    "dpo_lengths_values = [length for length, _ in dpo_lengths]\n",
    "sft_lengths_values = [length for length, _ in sft_lengths]\n",
    "\n",
    "plt.hist(dpo_lengths_values, bins=100, alpha=0.5, label='DPO lengths', range=(-1000, 5000))\n",
    "plt.hist(sft_lengths_values, bins=100, alpha=0.5, label='SFT lengths', range=(-1000, 5000))\n",
    "plt.xlabel('Length in tokens')\n",
    "plt.ylabel('Number of data points')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Token Length Distribution')\n",
    "plt.xlim(-1000, 5000)\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage of data points exceeding 2048 tokens\n",
    "dpo_exceeding_2048 = sum(1 for length in dpo_lengths_values if length > 2048) / len(dpo_lengths_values) * 100\n",
    "sft_exceeding_2048 = sum(1 for length in sft_lengths_values if length > 2048) / len(sft_lengths_values) * 100\n",
    "\n",
    "print(f\"Percentage of DPO data points exceeding 2048 tokens: {dpo_exceeding_2048:.2f}%\")\n",
    "print(f\"Percentage of SFT data points exceeding 2048 tokens: {sft_exceeding_2048:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Set the input and output file paths\n",
    "input_file = \"datasets/SE_DPO_.jsonl\"\n",
    "train_file = \"datasets/SE_DPO_train.jsonl\"\n",
    "test_file = \"datasets/SE_DPO_test.jsonl\"\n",
    "\n",
    "# Set the split ratio (95% for training, 5% for testing)\n",
    "train_ratio = 0.95\n",
    "\n",
    "# Read the data from the input JSONL file\n",
    "with open(input_file, \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Shuffle the data randomly\n",
    "random.shuffle(data)\n",
    "\n",
    "# Calculate the split index\n",
    "split_index = int(len(data) * train_ratio)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "# Write the training data to the train JSONL file\n",
    "with open(train_file, \"w\") as file:\n",
    "    for item in train_data:\n",
    "        file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Write the testing data to the test JSONL file\n",
    "with open(test_file, \"w\") as file:\n",
    "    for item in test_data:\n",
    "        file.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Split complete. Training data: {len(train_data)} items, Testing data: {len(test_data)} items.\")\n",
    "\n",
    "\n",
    "\n",
    "import utils\n",
    "data = []\n",
    "for path in [\"datasets/processed_DS_SE_cleaned_DPO.jsonl\", \"datasets/processed_MATH_SE_cleaned_DPO.jsonl\",\n",
    "             \"datasets/processed_CS_SE_cleaned_DPO.jsonl\", \"datasets/processed_PHYSICS_SE_cleaned_DPO.jsonl\"] :\n",
    "\n",
    "    data = data + utils.read_jsonl(path)\n",
    "\n",
    "utils.write_jsonl(data, 'datasets/SE_DPO_.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import random as random\n",
    "def merge_jsonl(file1, file2, output_file, num_samples=10000):\n",
    "    data1 = utils.read_jsonl(file1)\n",
    "    data2 = utils.read_jsonl(file2)\n",
    "    \n",
    "    sample1 = random.sample(data1, num_samples)\n",
    "    sample2 = random.sample(data2, num_samples)\n",
    "    \n",
    "    merged_data = sample1 + sample2\n",
    "    random.shuffle(merged_data)  # Shuffle to ensure no order preference\n",
    "    \n",
    "    utils.write_jsonl(merged_data, output_file)\n",
    "\n",
    "# Usage\n",
    "file1 = 'datasets/M1_2000_no_mcqa_train_correct.jsonl'\n",
    "file2 = 'datasets/SE_DPO_train_2000.jsonl'\n",
    "output_file = 'datasets/merged_DPO.jsonl'\n",
    "\n",
    "merge_jsonl(file1, file2, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'datasets/M1_2000_no_mcqa_train_correct.jsonl'\n",
    "file2 = 'datasets/SE_DPO_train_2000.jsonl'\n",
    "output_file = 'datasets/merged_DPO_train.jsonl'\n",
    "\n",
    "merge_jsonl(file1, file2, output_file, num_samples=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'datasets/M1_2000_no_mcqa_test_correct.jsonl'\n",
    "file2 = 'datasets/SE_DPO_test_2000.jsonl'\n",
    "output_file = 'datasets/merged_DPO_test.jsonl'\n",
    "\n",
    "merge_jsonl(file1, file2, output_file, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate([1,2,3,4,5,6,7], 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCQA_merged = utils.read_jsonl('datasets/MCQA_annotated.jsonl.jsonl') + utils.read_jsonl('datasets/MCQA_annotated_2.jsonl.jsonl')\n",
    "first = 0\n",
    "last = 0\n",
    "counter_dict = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0}\n",
    "first_prompt = MCQA_merged[0]['prompt']\n",
    "\n",
    "for datapoint in MCQA_merged:\n",
    "    if first_prompt == datapoint['prompt']:\n",
    "        counter_dict[datapoint[\"chosen\"]]  += 1\n",
    "        last = last + 1\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        first_prompt = datapoint['prompt']\n",
    "        counter_dict = {\"A\": 0, \"B\": 0, \"C\": 0, \"D\": 0}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqa = utils.read_jsonl('datasets/MCQA_annotated.jsonl') + utils.read_jsonl('datasets/MCQA_annotated_2.jsonl')\n",
    "mcqa[0]\n",
    "alt = {chr(i): 0 for i in range(ord('A'), ord('Z')+1)}\n",
    "new_mcqa = []\n",
    "for i in range(1,len(mcqa)):\n",
    "    if mcqa[i][\"prompt\"] == mcqa[i-1][\"prompt\"]:\n",
    "        alt[mcqa[i][\"chosen\"].upper()] += 1\n",
    "    else: \n",
    "        max_alt = max(alt, key=alt.get)\n",
    "        new_mcqa.append({\"question\": mcqa[i-1][\"prompt\"], \"answer\": max_alt})\n",
    "        alt = {chr(i): 0 for i in range(ord('A'), ord('Z')+1)}\n",
    "\n",
    "new_mcqa.append({\"question\": mcqa[-1][\"prompt\"], \"answer\": max_alt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_jsonl(new_mcqa, 'datasets/MCQA_unique_data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqa = utils.read_jsonl('datasets/MCQA_annotated.jsonl') + utils.read_jsonl('datasets/MCQA_annotated_2.jsonl')\n",
    "\n",
    "first = 0\n",
    "last = 0\n",
    "alt = {chr(i): 0 for i in range(ord('A'), ord('Z')+1)}\n",
    "for i in range(1, len(mcqa)):\n",
    "    if mcqa[i][\"prompt\"] == mcqa[i-1][\"prompt\"]:\n",
    "        alt[mcqa[i][\"chosen\"].upper()] += 1\n",
    "        last = i\n",
    "    else:\n",
    "        max_alt = max(alt, key=alt.get) \n",
    "        for j in range(first,last+1):\n",
    "            mcqa[j][\"chosen\"] = max_alt\n",
    "        alt = {chr(i): 0 for i in range(ord('A'), ord('Z')+1)}\n",
    "        first = i\n",
    "        \n",
    "max_alt = max(alt, key=alt.get)\n",
    "for j in range(first,last):\n",
    "    mcqa[j][\"chosen\"] = max_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_jsonl(mcqa, 'datasets/MCQA_correct_answer.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
